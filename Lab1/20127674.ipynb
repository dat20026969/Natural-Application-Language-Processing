{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cách xử lý giới hạn tốc độ:\n",
        "Tránh và xử lí khi bị, do quá nhiều yêu cầu truy cập API hoặc đã bị giới hạn tốc độ.\n",
        "Vì sao lại giới hạn?\n",
        "\n",
        "1. Vì chúng giúp bảo vệ chống lạm dụng hoặc sử dụng sai API.\n",
        "2. Giới hạn tỷ lệ giúp đảm bảo rằng mọi người đều có quyền truy cập công bằng vào API.\n",
        "3. Giới hạn tốc độ có thể giúp OpenAI quản lý tải tổng hợp trên cơ sở hạ tầng của nó.\n",
        "Mặc dù việc đạt đến giới hạn tốc độ có thể gây khó chịu, nhưng giới hạn tốc độ tồn tại để bảo vệ hoạt động đáng tin cậy của API cho người dùng."
      ],
      "metadata": {
        "id": "qSBiZZVwxsuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giới hạn tốc độ mặc định\n",
        "Có sự khác biệt giữa người dùng thử, người trả tiền và sử dụng trước, và sau 48 tiếng đồng hồ. Rõ ràng càng sử dụng lâu thì yêu cầu càng nhiều, có thể dẫn đến lỗi 429-Quá nhiều lượt truy cập hoặc lỗi giới hạn tỷ lệ và có thể dẫn đến quá tải nếu không giới hạn tốc độ truy cập/yêu cầu API OpenAI."
      ],
      "metadata": {
        "id": "HTsBK_jpyGGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install backoff"
      ],
      "metadata": {
        "id": "Mp3BGTRuyILg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mô-đun backoff cung cấp các công cụ trang trí chức năng có thể được sử dụng để bọc một chức năng sao cho chức năng đó sẽ được thử lại cho đến khi một số điều kiện được đáp ứng. Nó có nghĩa là được sử dụng khi truy cập các tài nguyên không đáng tin cậy có khả năng xảy ra lỗi không liên tục, tức là tài nguyên mạng và API bên ngoài. Tổng quát hơn một chút, nó cũng có thể được sử dụng cho các tài nguyên thăm dò ý kiến ​​động đối với nội dung được tạo bên ngoài."
      ],
      "metadata": {
        "id": "DUVqDmQaDzFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "bE3CDRghVUSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import backoff\n",
        "import openai\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "ZqEvRMenySwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = (\"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\")"
      ],
      "metadata": {
        "id": "I8WwgbvugHC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kích hoạt lỗi giới hạn tốc độ.\n",
        "# yêu cầu một loạt các lần hoàn thành trong một vòng lặp\n",
        "for _ in range(100):\n",
        "    openai.Completion.create(\n",
        "        model=\"code-cushman-001\",\n",
        "        prompt=\"def magic_function():\\n\\t\",\n",
        "        max_tokens=10,)\n",
        "# Đây là một đoạn code Python sử dụng thư viện OpenAI để tạo ra 100 lần yêu cầu đối với mô hình \"code-cushman-001\" \n",
        "# để hoàn thành câu lệnh \"def magic_function():\\n\\t\" bằng tối đa 10 tokens (các từ hoặc ký tự). Mô hình \"code-cushman-001\" \n",
        "# được đào tạo để tạo ra các đoạn mã Python có thể hoàn thành từ các đầu vào nhất định. Tuy nhiên, do chỉ cung cấp một \n",
        "# đoạn mã mẫu rất ngắn, nên kết quả có thể không có ý nghĩa gì hoặc không đầy đủ để thực sự hoàn thành một đoạn mã \n",
        "\n"
      ],
      "metadata": {
        "id": "jleM41czyUhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mô hình bị ảnh hưởng\n",
        "Các mẫu sau sẽ ngừng sản xuất <23/03/2023>:\n",
        "- code-cushman:001\n",
        "- code-cushman:002\n",
        "- code-davinci:001\n",
        "- code-davinci:002\n",
        "\n",
        "Các mẫu mã \"code-cushman-001\", \"code-cushman:002\", \"code-davinci:001\" và \"code-davinci:002\" được sử dụng trong OpenAI để đặt tên cho các phiên bản của các thuật toán và mô hình AI của họ.\n",
        "\n",
        "\"code-cushman\" và \"code-davinci\" là tên của các mô hình AI của OpenAI. Mã số ở cuối (001, 002, ...) thường được sử dụng để đánh giá các phiên bản khác nhau của một mô hình hoặc thuật toán.\n",
        "\n",
        "Ví dụ, mô hình AI GPT-3 của OpenAI có nhiều phiên bản khác nhau được gán các mã khác nhau để phân biệt chúng, ví dụ: \"davinci\", \"curie\", \"babbage\", \"ada\",... Trong đó, \"davinci\" là phiên bản lớn nhất và mạnh nhất của GPT-3. Các phiên bản khác nhau của GPT-3 được gán các mã số khác nhau để phân biệt chúng và đánh giá hiệu suất của chúng.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rb2HH2F0a-e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cách tránh lỗi giới hạn tốc độ.\n",
        "\n",
        "1. Thử lại với dự phòng theo cấp số nhân.\n",
        "\n",
        "Có nghĩa là thực hiện một \"giấc ngủ ngắn\" khi xảy ra lỗi giới hạn tốc độ, sau đó thử lại yêu cầu không thành công. Nếu yêu cầu vẫn không thành công, thời lượng ngủ sẽ tăng lên và quá trình này được lặp lại. Điều này tiếp tục cho đến khi yêu cầu thành công hoặc cho đến khi đạt đến số lần thử lại tối đa.\n",
        "\n",
        "Cách tiếp cận này có nhiều lợi ích:\n",
        "\n",
        "Thử lại tự động có nghĩa là bạn có thể khôi phục từ các lỗi giới hạn tốc độ mà không gặp sự cố hoặc thiếu dữ liệu.\n",
        "Dự phòng theo cấp số nhân có nghĩa là các lần thử lại đầu tiên của bạn có thể được thử nhanh chóng, trong khi vẫn được hưởng lợi từ độ trễ lâu hơn nếu một vài lần thử lại đầu tiên của bạn không thành công.\n",
        "Thêm jitter ngẫu nhiên vào độ trễ giúp thử lại tất cả các cú đánh cùng một lúc\n",
        "Lưu ý rằng các yêu cầu không thành công góp phần vào giới hạn mỗi phút của bạn, do đó, việc gửi lại yêu cầu liên tục sẽ không hiệu quả.\n",
        "\n",
        "Dưới đây là một vài giải pháp ví dụ."
      ],
      "metadata": {
        "id": "IarO2LbMyf7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VD1: Sử dụng thư viện Tenacity. Đây là thư viện/công cụ của bên thứ 3 và OpenAI không đảm bảo về độ tin cậy hoặc bảo mật của nó.\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def completion_with_backoff(**kwargs):\n",
        "    return openai.Completion.create(**kwargs)\n",
        "completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n",
        "# Dòng code trên sử dụng thư viện tenacity để thực hiện exponential backoff \n",
        "# (là một thuật toán sử dụng phản hồi để giảm nhân lên tỷ lệ của một số quy trình, nhằm dần dần tìm ra tỷ lệ có thể chấp nhận được.\n",
        "# trong khi gọi API openai.Completion.create() để tạo ra một đoạn văn bản dựa trên prompt được cung cấp và mô hình được chỉ định.\n",
        "# Cụ thể, hàm completion_with_backoff() được định nghĩa để gọi API openai.Completion.create() với các đối số được truyền vào dưới dạng kwargs (keyword arguments). Tuy nhiên, để xử lý các tình huống lỗi \n",
        "# và giảm thiểu tác động đến hệ thống, hàm này sử dụng @retry decorator của thư viện tenacity.\n",
        "# Các tham số của @retry được cấu hình như sau:\n",
        "# wait_random_exponential(min=1, max=60): chờ một thời gian ngẫu nhiên giữa 1 và 60 giây, thời gian chờ này tăng theo lũy thừa của 2 sau mỗi lần thử lại.\n",
        "# stop_after_attempt(6): dừng lại sau 6 lần thử nếu không thành công.\n",
        "# Sau đó, hàm completion_with_backoff() được gọi với mô hình được chỉ định là text-davinci-002 và prompt là Once upon a time,. Nếu API gọi không thành công trong 6 lần thử, nó sẽ dừng lại và raise lỗi."
      ],
      "metadata": {
        "id": "RnJ3DOFSyiaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7d3143-b1a6-4906-e627-4ac63ad852e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-72Um0VYueQdGb8PxAnPnY5YXnCdAL at 0x7f53c8746c20> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \" before forever ago, there was an earth fairy that was tasked with giving gifts to\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1680829304,\n",
              "  \"id\": \"cmpl-72Um0VYueQdGb8PxAnPnY5YXnCdAL\",\n",
              "  \"model\": \"text-davinci-002\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 16,\n",
              "    \"prompt_tokens\": 5,\n",
              "    \"total_tokens\": 21\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E0XdRO3ROm-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VD2: Sử dụng thư viện Backoff. Tương tự với việc sử dụng thư viện Tenacity.\n",
        "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
        "def completions_with_backoff(**kwargs):\n",
        "    return openai.Completion.create(**kwargs)\n",
        "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n",
        "# Dòng code trên sử dụng thư viện backoff để thực hiện exponential backoff khi xảy ra lỗi RateLimitError trong khi gửi yêu cầu đến API của OpenAI.\n",
        "# Cụ thể, @backoff.on_exception() là một decorator để áp dụng exponential backoff cho một hàm cụ thể và chạy lại nếu xảy ra exception được chỉ định trong tham số đầu vào.\n",
        "# Ở đây, backoff.expo được sử dụng để thực hiện exponential backoff và openai.error.RateLimitError là exception được chỉ định.\n",
        "# Hàm completions_with_backoff() được sử dụng để gửi yêu cầu đến API của OpenAI với các tham số được truyền vào. Nếu xảy ra lỗi RateLimitError, hàm sẽ thực hiện \n",
        "# exponential backoff và thực hiện lại yêu cầu.\n",
        "# Cuối cùng, completions_with_backoff() được gọi với các tham số model và prompt để thực hiện yêu cầu tạo ra các completion từ mô hình text-davinci-002 với prompt cho trước."
      ],
      "metadata": {
        "id": "B-i4-ri0ym4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80403e27-c77a-4dd7-9f1d-4a4e3239c41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-72UmBOw5V3JLh6znTZAD9ybqpgY5J at 0x7f53c87464a0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \" there was a group of scholars who wanted to gain a better understanding of the lives\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1680829315,\n",
              "  \"id\": \"cmpl-72UmBOw5V3JLh6znTZAD9ybqpgY5J\",\n",
              "  \"model\": \"text-davinci-002\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 16,\n",
              "    \"prompt_tokens\": 5,\n",
              "    \"total_tokens\": 21\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VD3: Triển khai backoff 1 cách thủ công\n",
        "# define a retry decorator\n",
        "def retry_with_exponential_backoff(\n",
        "    func,\n",
        "    initial_delay: float = 1,\n",
        "    exponential_base: float = 2,\n",
        "    jitter: bool = True,\n",
        "    max_retries: int = 10,\n",
        "    errors: tuple = (openai.error.RateLimitError,),\n",
        "):\n",
        "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        # Initialize variables\n",
        "        num_retries = 0\n",
        "        delay = initial_delay\n",
        "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
        "        while True:\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "            # Retry on specified errors\n",
        "            except errors as e:\n",
        "                # Increment retries\n",
        "                num_retries += 1\n",
        "                # Check if max retries has been reached\n",
        "                if num_retries > max_retries:\n",
        "                    raise Exception(\n",
        "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
        "                    )\n",
        "                # Increment the delay\n",
        "                delay *= exponential_base * (1 + jitter * random.random())\n",
        "                # Sleep for the delay\n",
        "                time.sleep(delay)\n",
        "            # Raise exceptions for any errors not specified\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "    return wrapper\n",
        "@retry_with_exponential_backoff\n",
        "def completions_with_backoff(**kwargs):\n",
        "    return openai.Completion.create(**kwargs)\n",
        "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n",
        "# Đoạn code trên định nghĩa một hàm decorator retry_with_exponential_backoff nhận một hàm func là một function khác\n",
        "# và trả về một wrapper function (Hàm bao bọc là một hàm trong thư viện phần mềm hoặc chương trình máy tính có mục đích\n",
        "# chính là gọi một chương trình con thứ hai hoặc một lệnh gọi hệ thống mà không cần hoặc có ít tính toán bổ sung.)\n",
        "# được sử dụng để thực thi func với chiến lược thử lại với exponential backoff.\n",
        "# initial_delay là thời gian chờ trước khi thực hiện lần đầu tiên, mặc định là 1 giây.\n",
        "# exponential_base là cơ số của exponential backoff, mặc định là 2.\n",
        "# jitter xác định xem có thêm một yếu tố ngẫu nhiên trong thời gian chờ không, mặc định là True.\n",
        "# max_retries là số lần thử lại tối đa trước khi quyết định kết thúc, mặc định là 10.\n",
        "# errors là tuple chứa các loại lỗi sẽ được thử lại, mặc định là (openai.error.RateLimitError,).\n",
        "# Hàm decorator retry_with_exponential_backoff thực thi function func được truyền vào bên trong wrapper function. \n",
        "# Nếu có bất kỳ lỗi trong errors xảy ra, wrapper function sẽ thử lại func với thời gian chờ được tính dựa trên cơ số exponential_base \n",
        "# và thời gian chờ ban đầu initial_delay. Thời gian chờ càng lớn sau mỗi lần thất bại vì cơ số exponential, điều này giúp tránh tình trạng \n",
        "# overload cho máy chủ API. Nếu max_retries lần thử lại đã được thực hiện mà vẫn không thành công, ngoại lệ Exception sẽ được ném ra. \n",
        "# Nếu bất kỳ lỗi không được liệt kê trong errors xảy ra, ngoại lệ sẽ được ném ra.\n",
        "# Ở đây, hàm decorator retry_with_exponential_backoff được sử dụng để bọc hàm completions_with_backoff,\n",
        "# từ đó, mỗi khi hàm completions_with_backoff được gọi, nó sẽ được thực thi lại với chiến lược thử lại exponential backoff. \n",
        "# Sau đó, hàm completions_with_backoff sử dụng OpenAI API để tạo ra một đoạn văn bản dựa trên model được chỉ định và prompt cho trước."
      ],
      "metadata": {
        "id": "_j9Efn55ypa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de92f596-38b6-481a-f4e7-89f4beb7babe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-72UmfH1ZLGXK28cTToEUrqaFcSaep at 0x7f53c8b62b30> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \" there was a lost civilization of men who lived without law or religion, and their\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1680829345,\n",
              "  \"id\": \"cmpl-72UmfH1ZLGXK28cTToEUrqaFcSaep\",\n",
              "  \"model\": \"text-davinci-002\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 16,\n",
              "    \"prompt_tokens\": 5,\n",
              "    \"total_tokens\": 21\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cách tối đa hóa thông lượng xử lý hàng loạt với các giới hạn tốc độ đã cho\n",
        "\n",
        "Lùi lại và thử lại là một chiến lược tuyệt vời để giảm thiểu độ trễ đồng thời tránh các lỗi giới hạn tốc độ.\n",
        "\n",
        "Tuy nhiên, nếu bạn đang xử lý khối lượng lớn dữ liệu lô, trong đó thông lượng quan trọng hơn độ trễ, bạn có thể thực hiện một số việc khác ngoài việc lùi và thử lại.\n",
        "\n",
        "Chủ động thêm độ trễ giữa các yêu cầu\n",
        "\n",
        "Ở đây, một giải pháp tiềm năng là tính toán giới hạn tốc độ của bạn và thêm độ trễ bằng với đối ứng của nó (ví dụ: nếu giới hạn tốc độ của bạn là 20 yêu cầu mỗi phút, hãy thêm độ trễ từ 3–6 giây cho mỗi yêu cầu). Điều này có thể giúp bạn vận hành gần mức trần giới hạn tốc độ mà không đạt đến mức đó và phát sinh các yêu cầu lãng phí."
      ],
      "metadata": {
        "id": "8QCKEuuqyvw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ví dụ về việc thêm độ trễ vào yêu cầu\n",
        "# Define a function that adds a delay to a Completion API call\n",
        "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
        "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
        "    # Sleep for the delay\n",
        "    time.sleep(delay_in_seconds)\n",
        "    # Call the Completion API and return the result\n",
        "    return openai.Completion.create(**kwargs)\n",
        "# Calculate the delay based on your rate limit\n",
        "rate_limit_per_minute = 20\n",
        "delay = 60.0 / rate_limit_per_minute\n",
        "delayed_completion(\n",
        "    delay_in_seconds=delay,\n",
        "    model=\"text-davinci-002\",\n",
        "    prompt=\"Once upon a time,\"\n",
        ")\n",
        "# Đoạn code này có chức năng tạo ra một hàm delayed_completion với đầu vào là thời gian trễ (delay_in_seconds) \n",
        "# và các tham số khác được truyền cho API OpenAI Completion. Khi gọi hàm delayed_completion, nó sẽ dừng chương trình\n",
        "# trong một khoảng thời gian xác định (dựa trên giới hạn tốc độ của bạn) trước khi gọi API OpenAI Completion\n",
        "# để tránh vượt quá giới hạn tốc độ và nhận một kết quả từ API. Trong trường hợp này, giá trị của delay được tính\n",
        "# dựa trên giới hạn tốc độ là 20 lượt/phút và được truyền vào delayed_completion cùng với các thông số khác để gọi API OpenAI Completion."
      ],
      "metadata": {
        "id": "Ebrzsd_vy3Zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3127f34a-bab4-4fbc-e384-9ddd65597117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-72UoPf6bR90cxwewFGVS6lBFs2Ak4 at 0x7f53c8747f40> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \" the universe was black,'' he said in his posthumously published `'St\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1680829453,\n",
              "  \"id\": \"cmpl-72UoPf6bR90cxwewFGVS6lBFs2Ak4\",\n",
              "  \"model\": \"text-davinci-002\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 16,\n",
              "    \"prompt_tokens\": 5,\n",
              "    \"total_tokens\": 21\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Yêu cầu hàng hoạt:\n",
        "Nếu bạn đang đạt đến giới hạn về số yêu cầu mỗi phút, nhưng có khoảng trống về số mã thông báo mỗi phút, thì bạn có thể tăng thông lượng của mình bằng cách gộp nhiều tác vụ vào từng yêu cầu. Điều này sẽ cho phép bạn xử lý nhiều mã thông báo hơn mỗi phút, đặc biệt là với các mô hình nhỏ hơn.\n",
        "\n",
        "Việc gửi một loạt lời nhắc hoạt động giống hệt như một lệnh gọi API thông thường, ngoại trừ việc chuyển một danh sách các chuỗi thành tham số lời nhắc thay vì một chuỗi."
      ],
      "metadata": {
        "id": "OhYqOf3Ly-IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ví dụ về không yêu cầu hàng loạt:\n",
        "num_stories = 10\n",
        "prompt = \"Once upon a time,\"\n",
        "# serial example, with one story completion per request\n",
        "for _ in range(num_stories):\n",
        "    response = openai.Completion.create(\n",
        "        model=\"curie\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=20,\n",
        "    )\n",
        "    # print story\n",
        "    print(prompt + response.choices[0].text)\n",
        "# Đoạn code trên có chức năng tạo ra num_stories câu chuyện bắt đầu bằng prompt sử dụng model curie. \n",
        "# Mỗi câu chuyện được tạo ra thông qua một API request đến OpenAI API bằng hàm openai.Completion.create(). \n",
        "# Mỗi câu chuyện được giới hạn bởi số lượng ký tự tối đa được định nghĩa bởi max_tokens. Sau đó, câu chuyện được in ra màn hình thông qua lệnh print()."
      ],
      "metadata": {
        "id": "MyxINpYOy_4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6424a47-451d-4434-9c65-cc66c79b5cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, before I had a blog, I wrote an article on how much I hate Paypal. I wrote\n",
            "Once upon a time, my father lived next door to a boy who went to the same school as me until he went to\n",
            "Once upon a time, the government of post-apartheid South Africa (GOSA) tried to sell the whites\n",
            "Once upon a time, there was a story about Nick Taylor, born shortly after 10th April 1932, who grew up to\n",
            "Once upon a time, people used to travel from one village to another in order to go to the village situated opposite to theirs\n",
            "Once upon a time, this trail was fantastic. Now it feels diminished, even for a short. He makes an impromptu\n",
            "Once upon a time, there lived a king\n",
            "\n",
            "who dreamed of owning a huge castle.\n",
            "\n",
            "One day he passed\n",
            "Once upon a time, Abraham and Norman nearly came to blows over this nutty bird, but though it happened to be that\n",
            "Once upon a time, Italian folktales of Greek origin became the very favorite subject for all Italian composers from the bass\n",
            "Once upon a time, air conditioners worked with the hot coolant, but with the use of refrigerant, we will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Và khi yêu cầu hàng loạt:\n",
        "num_stories = 10\n",
        "prompts = [\"Once upon a time,\"] * num_stories\n",
        "# batched example, with 10 stories completions per request\n",
        "response = openai.Completion.create(\n",
        "    model=\"curie\",\n",
        "    prompt=prompts,\n",
        "    max_tokens=20,\n",
        ")\n",
        "# match completions to prompts by index\n",
        "stories = [\"\"] * len(prompts)\n",
        "for choice in response.choices:\n",
        "    stories[choice.index] = prompts[choice.index] + choice.text\n",
        "# print stories\n",
        "for story in stories:\n",
        "    print(story)\n",
        "# Đoạn code trên sử dụng API của OpenAI để tạo ra một danh sách gồm 10 câu chuyện (story) với đoạn prompt là \"Once upon a time,\" \n",
        "# và mỗi câu chuyện có tối đa 20 từ. Khác với đoạn code trước đó, ở đây việc tạo các câu chuyện được thực hiện\n",
        "# thông qua việc gửi một batch gồm 10 prompt cho API. Sau khi nhận được phản hồi, chương trình sẽ khớp các câu trả lời với prompt tương ứng\n",
        "# thông qua chỉ số index và sau đó in ra các câu chuyện tương ứng."
      ],
      "metadata": {
        "id": "APM0zGKozGKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df55ef1-37e0-483c-86b6-1efd6e47cde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, some years ago, President Bill Clinton changed our U.S. Immigration laws by executive order that allowed\n",
            "Once upon a time, a creature called a Cyclops made his home at the far end of this very cave. The creature\n",
            "Once upon a time, I was in the midst of finding my life’s purpose. I had no clear idea what\n",
            "Once upon a time, there was a little pig, who was willing to do anything to get her own way and who,\n",
            "Once upon a time, I combed the internet for building research and spoke to professionals for an answer. What I found was\n",
            "Once upon a time, you could confuse Zelda with Frozen, work out something was wrong and start on a crusade to castrate\n",
            "Once upon a time, you could get your regular colonoscopy. Now, it is more of a gamble. You could\n",
            "Once upon a time, many years ago, in a far away land, lived a boy called Tora. Tora was\n",
            "Once upon a time,” said Thaler, “I wrote a manifesto on how life is and how it should\n",
            "Once upon a time, in a little workshop in the woods of Tennessee, there were two friends. One of those friends was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ví dụ tập lệnh xử lí song song.\n",
        "import aiohttp  # for making API calls concurrently\n",
        "import argparse  # for running script from command line\n",
        "import asyncio  # for running API calls concurrently\n",
        "import json  # for saving results to a jsonl file\n",
        "import logging  # for logging rate limit warnings and other messages\n",
        "import os  # for reading API key\n",
        "import re  # for matching endpoint from request URL\n",
        "import tiktoken  # for counting tokens\n",
        "import time  # for sleeping after rate limit is hit\n",
        "import sys\n",
        "from sys import argv\n",
        "from dataclasses import dataclass  # for storing API inputs, outputs, and metadata\n",
        "\n",
        "\n",
        "async def process_api_requests_from_file(\n",
        "    requests_filepath: str,\n",
        "    save_filepath: str,\n",
        "    request_url: str,\n",
        "    api_key: str,\n",
        "    max_requests_per_minute: float,\n",
        "    max_tokens_per_minute: float,\n",
        "    token_encoding_name: str,\n",
        "    max_attempts: int,\n",
        "    logging_level: int,\n",
        "):\n",
        "    \"\"\"Processes API requests in parallel, throttling to stay under rate limits.\"\"\"\n",
        "    # constants\n",
        "    seconds_to_pause_after_rate_limit_error = 15\n",
        "    seconds_to_sleep_each_loop = 0.001  # 1 ms limits max throughput to 1,000 requests per second\n",
        "\n",
        "    # initialize logging\n",
        "    logging.basicConfig(level=logging_level)\n",
        "    logging.debug(f\"Logging initialized at level {logging_level}\")\n",
        "\n",
        "    # infer API endpoint and construct request header\n",
        "    api_endpoint = api_endpoint_from_url(request_url)\n",
        "    request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    # initialize trackers\n",
        "    queue_of_requests_to_retry = asyncio.Queue()\n",
        "    task_id_generator = task_id_generator_function()  # generates integer IDs of 1, 2, 3, ...\n",
        "    status_tracker = StatusTracker()  # single instance to track a collection of variables\n",
        "    next_request = None  # variable to hold the next request to call\n",
        "\n",
        "    # initialize available capacity counts\n",
        "    available_request_capacity = max_requests_per_minute\n",
        "    available_token_capacity = max_tokens_per_minute\n",
        "    last_update_time = time.time()\n",
        "\n",
        "    # initialize flags\n",
        "    file_not_finished = True  # after file is empty, we'll skip reading it\n",
        "    logging.debug(f\"Initialization complete.\")\n",
        "\n",
        "    # initialize file reading\n",
        "    with open(requests_filepath) as file:\n",
        "        # `requests` will provide requests one at a time\n",
        "        requests = file.__iter__()\n",
        "        logging.debug(f\"File opened. Entering main loop\")\n",
        "\n",
        "        while True:\n",
        "            # get next request (if one is not already waiting for capacity)\n",
        "            if next_request is None:\n",
        "                if not queue_of_requests_to_retry.empty():\n",
        "                    next_request = queue_of_requests_to_retry.get_nowait()\n",
        "                    logging.debug(f\"Retrying request {next_request.task_id}: {next_request}\")\n",
        "                elif file_not_finished:\n",
        "                    try:\n",
        "                        # get new request\n",
        "                        request_json = json.loads(next(requests))\n",
        "                        next_request = APIRequest(\n",
        "                            task_id=next(task_id_generator),\n",
        "                            request_json=request_json,\n",
        "                            token_consumption=num_tokens_consumed_from_request(request_json, api_endpoint, token_encoding_name),\n",
        "                            attempts_left=max_attempts,\n",
        "                        )\n",
        "                        status_tracker.num_tasks_started += 1\n",
        "                        status_tracker.num_tasks_in_progress += 1\n",
        "                        logging.debug(f\"Reading request {next_request.task_id}: {next_request}\")\n",
        "                    except StopIteration:\n",
        "                        # if file runs out, set flag to stop reading it\n",
        "                        logging.debug(\"Read file exhausted\")\n",
        "                        file_not_finished = False\n",
        "\n",
        "            # update available capacity\n",
        "            current_time = time.time()\n",
        "            seconds_since_update = current_time - last_update_time\n",
        "            available_request_capacity = min(\n",
        "                available_request_capacity + max_requests_per_minute * seconds_since_update / 60.0,\n",
        "                max_requests_per_minute,\n",
        "            )\n",
        "            available_token_capacity = min(\n",
        "                available_token_capacity + max_tokens_per_minute * seconds_since_update / 60.0,\n",
        "                max_tokens_per_minute,\n",
        "            )\n",
        "            last_update_time = current_time\n",
        "\n",
        "            # if enough capacity available, call API\n",
        "            if next_request:\n",
        "                next_request_tokens = next_request.token_consumption\n",
        "                if (\n",
        "                    available_request_capacity >= 1\n",
        "                    and available_token_capacity >= next_request_tokens\n",
        "                ):\n",
        "                    # update counters\n",
        "                    available_request_capacity -= 1\n",
        "                    available_token_capacity -= next_request_tokens\n",
        "                    next_request.attempts_left -= 1\n",
        "\n",
        "                    # call API\n",
        "                    asyncio.create_task(\n",
        "                        next_request.call_api(\n",
        "                            request_url=request_url,\n",
        "                            request_header=request_header,\n",
        "                            retry_queue=queue_of_requests_to_retry,\n",
        "                            save_filepath=save_filepath,\n",
        "                            status_tracker=status_tracker,\n",
        "                        )\n",
        "                    )\n",
        "                    next_request = None  # reset next_request to empty\n",
        "\n",
        "            # if all tasks are finished, break\n",
        "            if status_tracker.num_tasks_in_progress == 0:\n",
        "                break\n",
        "\n",
        "            # main loop sleeps briefly so concurrent tasks can run\n",
        "            await asyncio.sleep(seconds_to_sleep_each_loop)\n",
        "\n",
        "            # if a rate limit error was hit recently, pause to cool down\n",
        "            seconds_since_rate_limit_error = (time.time() - status_tracker.time_of_last_rate_limit_error)\n",
        "            if seconds_since_rate_limit_error < seconds_to_pause_after_rate_limit_error:\n",
        "                remaining_seconds_to_pause = (seconds_to_pause_after_rate_limit_error - seconds_since_rate_limit_error)\n",
        "                await asyncio.sleep(remaining_seconds_to_pause)\n",
        "                # ^e.g., if pause is 15 seconds and final limit was hit 5 seconds ago\n",
        "                logging.warn(f\"Pausing to cool down until {time.ctime(status_tracker.time_of_last_rate_limit_error + seconds_to_pause_after_rate_limit_error)}\")\n",
        "\n",
        "        # after finishing, log final status\n",
        "        logging.info(f\"\"\"Parallel processing complete. Results saved to {save_filepath}\"\"\")\n",
        "        if status_tracker.num_tasks_failed > 0:\n",
        "            logging.warning(f\"{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed. Errors logged to {save_filepath}.\")\n",
        "        if status_tracker.num_rate_limit_errors > 0:\n",
        "            logging.warning(f\"{status_tracker.num_rate_limit_errors} rate limit errors received. Consider running at a lower rate.\")\n",
        "\n",
        "\n",
        "# dataclasses\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class StatusTracker:\n",
        "    \"\"\"Stores metadata about the script's progress. Only one instance is created.\"\"\"\n",
        "\n",
        "    num_tasks_started: int = 0\n",
        "    num_tasks_in_progress: int = 0  # script ends when this reaches 0\n",
        "    num_tasks_succeeded: int = 0\n",
        "    num_tasks_failed: int = 0\n",
        "    num_rate_limit_errors: int = 0\n",
        "    num_api_errors: int = 0  # excluding rate limit errors, counted above\n",
        "    num_other_errors: int = 0\n",
        "    time_of_last_rate_limit_error: int = 0  # used to cool off after hitting rate limits\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class APIRequest:\n",
        "    \"\"\"Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API call.\"\"\"\n",
        "\n",
        "    task_id: int\n",
        "    request_json: dict\n",
        "    token_consumption: int\n",
        "    attempts_left: int\n",
        "    result = []\n",
        "\n",
        "    async def call_api(\n",
        "        self,\n",
        "        request_url: str,\n",
        "        request_header: dict,\n",
        "        retry_queue: asyncio.Queue,\n",
        "        save_filepath: str,\n",
        "        status_tracker: StatusTracker,\n",
        "    ):\n",
        "        \"\"\"Calls the OpenAI API and saves results.\"\"\"\n",
        "        logging.info(f\"Starting request #{self.task_id}\")\n",
        "        error = None\n",
        "        try:\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.post(\n",
        "                    url=request_url, headers=request_header, json=self.request_json\n",
        "                ) as response:\n",
        "                    response = await response.json()\n",
        "            if \"error\" in response:\n",
        "                logging.warning(\n",
        "                    f\"Request {self.task_id} failed with error {response['error']}\"\n",
        "                )\n",
        "                status_tracker.num_api_errors += 1\n",
        "                error = response\n",
        "                if \"Rate limit\" in response[\"error\"].get(\"message\", \"\"):\n",
        "                    status_tracker.time_of_last_rate_limit_error = time.time()\n",
        "                    status_tracker.num_rate_limit_errors += 1\n",
        "                    status_tracker.num_api_errors -= 1  # rate limit errors are counted separately\n",
        "\n",
        "        except Exception as e:  # catching naked exceptions is bad practice, but in this case we'll log & save them\n",
        "            logging.warning(f\"Request {self.task_id} failed with Exception {e}\")\n",
        "            status_tracker.num_other_errors += 1\n",
        "            error = e\n",
        "        if error:\n",
        "            self.result.append(error)\n",
        "            if self.attempts_left:\n",
        "                retry_queue.put_nowait(self)\n",
        "            else:\n",
        "                logging.error(f\"Request {self.request_json} failed after all attempts. Saving errors: {self.result}\")\n",
        "                append_to_jsonl([self.request_json, self.result], save_filepath)\n",
        "                status_tracker.num_tasks_in_progress -= 1\n",
        "                status_tracker.num_tasks_failed += 1\n",
        "        else:\n",
        "            append_to_jsonl([self.request_json, response], save_filepath)\n",
        "            status_tracker.num_tasks_in_progress -= 1\n",
        "            status_tracker.num_tasks_succeeded += 1\n",
        "            logging.debug(f\"Request {self.task_id} saved to {save_filepath}\")\n",
        "\n",
        "\n",
        "# functions\n",
        "\n",
        "\n",
        "def api_endpoint_from_url(request_url):\n",
        "    \"\"\"Extract the API endpoint from the request URL.\"\"\"\n",
        "    match = re.search('^https://[^/]+/v\\\\d+/(.+)$', request_url)\n",
        "    return match[1]\n",
        "\n",
        "\n",
        "def append_to_jsonl(data, filename: str) -> None:\n",
        "    \"\"\"Append a json payload to the end of a jsonl file.\"\"\"\n",
        "    json_string = json.dumps(data)\n",
        "    with open(filename, \"a\") as f:\n",
        "        f.write(json_string + \"\\n\")\n",
        "\n",
        "\n",
        "def num_tokens_consumed_from_request(\n",
        "    request_json: dict,\n",
        "    api_endpoint: str,\n",
        "    token_encoding_name: str,\n",
        "):\n",
        "    \"\"\"Count the number of tokens in the request. Only supports completion and embedding requests.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(token_encoding_name)\n",
        "    # if completions request, tokens = prompt + n * max_tokens\n",
        "    if api_endpoint.endswith(\"completions\"):\n",
        "        max_tokens = request_json.get(\"max_tokens\", 15)\n",
        "        n = request_json.get(\"n\", 1)\n",
        "        completion_tokens = n * max_tokens\n",
        "\n",
        "        # chat completions\n",
        "        if api_endpoint.startswith(\"chat/\"):\n",
        "            num_tokens = 0\n",
        "            for message in request_json[\"messages\"]:\n",
        "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "                for key, value in message.items():\n",
        "                    num_tokens += len(encoding.encode(value))\n",
        "                    if key == \"name\":  # if there's a name, the role is omitted\n",
        "                        num_tokens -= 1  # role is always required and always 1 token\n",
        "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "            return num_tokens + completion_tokens\n",
        "        # normal completions\n",
        "        else:\n",
        "            prompt = request_json[\"prompt\"]\n",
        "            if isinstance(prompt, str):  # single prompt\n",
        "                prompt_tokens = len(encoding.encode(prompt))\n",
        "                num_tokens = prompt_tokens + completion_tokens\n",
        "                return num_tokens\n",
        "            elif isinstance(prompt, list):  # multiple prompts\n",
        "                prompt_tokens = sum([len(encoding.encode(p)) for p in prompt])\n",
        "                num_tokens = prompt_tokens + completion_tokens * len(prompt)\n",
        "                return num_tokens\n",
        "            else:\n",
        "                raise TypeError('Expecting either string or list of strings for \"prompt\" field in completion request')\n",
        "    # if embeddings request, tokens = input tokens\n",
        "    elif api_endpoint == \"embeddings\":\n",
        "        input = request_json[\"input\"]\n",
        "        if isinstance(input, str):  # single input\n",
        "            num_tokens = len(encoding.encode(input))\n",
        "            return num_tokens\n",
        "        elif isinstance(input, list):  # multiple inputs\n",
        "            num_tokens = sum([len(encoding.encode(i)) for i in input])\n",
        "            return num_tokens\n",
        "        else:\n",
        "            raise TypeError('Expecting either string or list of strings for \"inputs\" field in embedding request')\n",
        "    # more logic needed to support other API calls (e.g., edits, inserts, DALL-E)\n",
        "    else:\n",
        "        raise NotImplementedError(f'API endpoint \"{api_endpoint}\" not implemented in this script')\n",
        "\n",
        "\n",
        "def task_id_generator_function():\n",
        "    \"\"\"Generate integers 0, 1, 2, and so on.\"\"\"\n",
        "    task_id = 0\n",
        "    while True:\n",
        "        yield task_id\n",
        "        task_id += 1\n",
        "\n",
        "\n",
        "# run script\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # parse command line arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
        "    parser.add_argument('--train_steps', default=1000, type=int, help='number of training steps')\n",
        "    parser.add_argument(\"--requests_filepath\")\n",
        "    parser.add_argument(\"--save_filepath\", default=None)\n",
        "    parser.add_argument(\"--request_url\", default=\"https://api.openai.com/v1/embeddings\")\n",
        "    parser.add_argument(\"--api_key\", default=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    parser.add_argument(\"--max_requests_per_minute\", type=int, default=3_000 * 0.5)\n",
        "    parser.add_argument(\"--max_tokens_per_minute\", type=int, default=250_000 * 0.5)\n",
        "    parser.add_argument(\"--token_encoding_name\", default=\"cl100k_base\")\n",
        "    parser.add_argument(\"--max_attempts\", type=int, default=5)\n",
        "    parser.add_argument(\"--logging_level\", default=logging.INFO)\n",
        "    args = parser.parse_args(argv[1:])\n",
        "\n",
        "    if args.save_filepath is None:\n",
        "        args.save_filepath = args.requests_filepath.replace(\".jsonl\", \"_results.jsonl\")\n",
        "\n",
        "    # run script\n",
        "    asyncio.run(\n",
        "        process_api_requests_from_file(\n",
        "            requests_filepath=args.requests_filepath,\n",
        "            save_filepath=args.save_filepath,\n",
        "            request_url=args.request_url,\n",
        "            api_key=args.api_key,\n",
        "            max_requests_per_minute=float(args.max_requests_per_minute),\n",
        "            max_tokens_per_minute=float(args.max_tokens_per_minute),\n",
        "            token_encoding_name=args.token_encoding_name,\n",
        "            max_attempts=int(args.max_attempts),\n",
        "            logging_level=int(args.logging_level),\n",
        "        )\n",
        "    )\n",
        "# Đoạn mã này định nghĩa một hàm Python có tên là process_api_requests_from_file xử lý các yêu cầu API được đọc từ một tệp.\n",
        "# Các yêu cầu được điều chỉnh để duy trì dưới giới hạn tốc độ do người gọi chỉ định. Hàm xử lý các yêu cầu song song\n",
        "# bằng cách sử dụng asyncio, cho phép thực thi mã I/O không chặn và đồng thời.\n",
        "# Hàm đọc một tệp chứa danh sách các yêu cầu API được mã hóa JSON và xử lý từng yêu cầu bằng cách tạo một yêu cầu HTTP \n",
        "# tới điểm cuối API. Nó tạo tiêu đề yêu cầu HTTP với mã thông báo Ủy quyền, được chuyển dưới dạng tham số. Các yêu cầu được \n",
        "# xử lý song song với số lượng yêu cầu tối đa nhất định mỗi phút và số lượng mã thông báo tối đa mỗi phút, đây cũng là các tham số.\n",
        "# Hàm này sử dụng một thể hiện của lớp dữ liệu StatusTracker để lưu trữ siêu dữ liệu về tiến trình của tập lệnh.\n",
        "# Chức năng này sẽ ghi thông tin và cảnh báo khi nó chạy. Hàm không trả về gì cả."
      ],
      "metadata": {
        "id": "_lSX2o8ozJJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Làm thế nào để đếm tokens bằng tiktoken\n",
        "\n",
        "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) là một fast open-source tokenizer được tạo bởi OpenAI.\n",
        "\n",
        "Cho một chuỗi văn bản (e.g., `\"tiktoken is great!\"`) và một encoding (e.g., `\"cl100k_base\"`), một tokenizer có thể chia chuỗi văn bản thành danh sách các tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
        "\n",
        "Việc tách chuỗi văn bản thành các tokens rất hữu ích vì mô hình GPT có thể nhìn thấy văn bản ở dưới dạng tokens. Việc biết có bao nhiêu tokens trong một chuỗi văn bản có thể cho bạn biết (a) liệu chuỗi đó có quá dài để mô hình văn bản xử lý hay không và (b) chi phí lệnh gọi API OpenAI là bao nhiêu (vì việc sử dụng được định giá theo tokens).\n",
        "\n",
        "\n",
        "## Encodings\n",
        "\n",
        "Mã hóa chỉ định cách văn bản được chuyển đổi thành tokens. \n",
        "Các mô hình khác nhau sử dụng mã hóa khác nhau\n",
        "\n",
        "`tiktoken` hỗ trợ 3 encodings được sử dụng bởi mô hình OpenAI:\n",
        "\n",
        "| Encoding name           | OpenAI models                                       |\n",
        "|-------------------------|-----------------------------------------------------|\n",
        "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |\n",
        "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
        "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
        "\n",
        "Bạn có thể truy xuất mã hóa cho một mô hình bằng cách sử dụng `tiktoken.encoding_for_model()` như sau:\n",
        "```python\n",
        "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "```\n",
        "\n",
        "Chú ý `p50k_base` trùng lặp đáng kể với `r50k_base` và đối với các ứng dụng không phải mã, chúng thường sẽ cung cấp các token giống nhau..\n",
        "\n",
        "## Tokenizer libraries by language\n",
        "\n",
        "For `cl100k_base` and `p50k_base` encodings:\n",
        "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
        "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken)\n",
        "\n",
        "For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\n",
        "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
        "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
        "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
        "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
        "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
        "\n",
        "(OpenAI không xác nhận hay bảo đảm cho các thư viện của bên thứ ba.)\n",
        "\n",
        "\n",
        "## Cách các chuỗi thường được mã hóa\n",
        "\n",
        "Trong tiếng Anh, mã thông báo thường có độ dài từ một ký tự đến một từ (ví dụ: `\"t\"` hoặc `\"great\"`), mặc dù ở một số ngôn ngữ, mã thông báo có thể ngắn hơn một ký tự hoặc dài hơn một từ. Các khoảng trắng thường được nhóm với phần đầu của các từ (ví dụ: `\" is\"` thay vì `\" is \"` hoặc `\" \"`+`\"is\"`). Bạn có thể nhanh chóng kiểm tra cách một chuỗi được mã hóa tại [OpenAI Tokenizer](https://beta.openai.com/tokenizer)."
      ],
      "metadata": {
        "id": "HzC9FLKYwIgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Install `tiktoken`\n",
        "\n",
        "If needed, install `tiktoken` with `pip`:"
      ],
      "metadata": {
        "id": "95UFZXuIwSd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tiktoken\n"
      ],
      "metadata": {
        "id": "McCIl5z3wWzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df179ca-7047-4877-ece1-8c0c2befbcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import `tiktoken`"
      ],
      "metadata": {
        "id": "pqPQBqSAwZlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "zYx8o9lZwb8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tải mã hóa\n",
        "\n",
        "Sử dụng `tiktoken.get_encoding()` để tải mã hóa theo tên.\n",
        "\n",
        "Lần đầu tiên chạy, nó sẽ yêu cầu kết nối internet để tải xuống. Các lần chạy sau sẽ không cần kết nối internet."
      ],
      "metadata": {
        "id": "513OgSEywenH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
      ],
      "metadata": {
        "id": "2ToeISQ5wis6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name."
      ],
      "metadata": {
        "id": "ajQKhVySwkvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "nyEIkZpiwmRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Chuyển văn bản thành tokens với `encoding.encode()`\n",
        "\n"
      ],
      "metadata": {
        "id": "XskKYTWFwn6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phương pháp `.encode()` sẽ chuyển đổi một chuỗi văn bản thành một danh sách các số nguyên tokens."
      ],
      "metadata": {
        "id": "tzaM5A0swqbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"tiktoken is great!\")\n"
      ],
      "metadata": {
        "id": "V3Ld7m0Fwsi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da69ce7f-6d01-4c25-c2b1-824344f9a8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 1609, 5963, 374, 2294, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đếm tokens bằng cách đếm độ dài của danh sách bằng  `.encode()` trả về."
      ],
      "metadata": {
        "id": "OChKVW_7wuI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "# Đoạn code này định nghĩa một hàm Python có tên là num_tokens_from_string. Hàm này nhận vào hai đối số:\n",
        "# string: Một chuỗi văn bản.\n",
        "# encoding_name: Tên của bộ mã hóa để sử dụng khi mã hóa chuỗi văn bản.\n",
        "# Hàm trả về số lượng token trong chuỗi văn bản đầu vào. Để tính số lượng token, hàm sử dụng tiktoken \n",
        "# để tìm bộ mã hóa tương ứng với encoding_name, sau đó sử dụng bộ mã hóa này để mã hóa chuỗi văn bản đầu\n",
        "# vào và đếm số lượng token trong chuỗi được mã hóa. Cuối cùng, hàm trả về số lượng token được tính toán."
      ],
      "metadata": {
        "id": "aE_HNojPwvqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n"
      ],
      "metadata": {
        "id": "xwv8BmiFwxom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa86f44e-9c61-43da-b228-b022db8287d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Chuyển đổi tokens thành văn bản `encoding.decode()`"
      ],
      "metadata": {
        "id": "DUTf-sZEw05R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`.decode()` chuyển đổi danh sách các số nguyên tokens thành một chuỗi."
      ],
      "metadata": {
        "id": "W2IRensFw2ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([83, 1609, 5963, 374, 2294, 0])\n"
      ],
      "metadata": {
        "id": "TGFzV31Ywy8_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6fca5e6a-9e53-4c83-b91e-2ecf5e19bccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiktoken is great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cảnh báo: mặc dù `.decode()` có thể được áp dụng cho các single tokens, hãy lưu ý rằng nó có thể bị mất đối với các token không nằm trên ranh giới utf-8."
      ],
      "metadata": {
        "id": "eA9kCQpgw5lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đối với single tokens, `.decode_single_token_bytes()` chuyển đổi một cách an toàn single integer token thành các byte mà nó đại diện."
      ],
      "metadata": {
        "id": "kPDJpaXFw6_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n"
      ],
      "metadata": {
        "id": "2TL16jiPw82s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fab409-68b2-4bec-8f04-2caa0521ea6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b't', b'ik', b'token', b' is', b' great', b'!']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(`b` phía trước các chuỗi cho biết rằng các chuỗi là chuỗi byte.)"
      ],
      "metadata": {
        "id": "g_b5MZ4Cw_dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Sự so sánh encodings\n",
        "\n",
        "Các mã hóa khác nhau khác nhau về cách chúng tách từ, nhóm khoảng trắng và xử lý các ký tự không phải tiếng Anh. Sử dụng các phương pháp trên, chúng ta có thể so sánh các mã hóa khác nhau trên một vài chuỗi ví dụ."
      ],
      "metadata": {
        "id": "85_ErZ1KxA6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # In ra chuỗi ví dụ\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # Mỗi cách mã hóa, in ra #  tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")\n",
        "# Đoạn code này định nghĩa một hàm compare_encodings nhận đầu vào là một chuỗi và in ra số lượng token\n",
        "# và mã hóa của chuỗi đó với ba kiểu mã hóa khác nhau là \"gpt2\", \"p50k_base\", \"cl100k_base\".\n",
        "# Trong mỗi vòng lặp, đoạn mã sử dụng tiktok để lấy mã hóa của kiểu mã hóa đang được xét và sử dụng \n",
        "# nó để mã hóa chuỗi đầu vào. Sau đó, nó tính toán số lượng token bằng cách đếm số lượng số nguyên trong\n",
        "# chuỗi mã hóa và tạo một danh sách các byte mã hóa cho từng token. Cuối cùng, đoạn mã in ra số lượng token\n",
        "# và các mã hóa tương ứng của chúng.        "
      ],
      "metadata": {
        "id": "8EIzZD9ZxCee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"antidisestablishmentarianism\")\n"
      ],
      "metadata": {
        "id": "LHh4ZnBRxEME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13796cce-8086-4b30-9e5d-d099aafd0832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"antidisestablishmentarianism\"\n",
            "\n",
            "gpt2: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "cl100k_base: 6 tokens\n",
            "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
            "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"2 + 2 = 4\")\n"
      ],
      "metadata": {
        "id": "F9eBU3h9xFoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91857a16-9238-4b6c-8940-e6fbe67fcfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "gpt2: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"お誕生日おめでとう\")\n"
      ],
      "metadata": {
        "id": "ArTBHStWxG9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbbd04e-eba7-455a-fde5-7ea3208ddd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"お誕生日おめでとう\"\n",
            "\n",
            "gpt2: 14 tokens\n",
            "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
            "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
            "\n",
            "p50k_base: 14 tokens\n",
            "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
            "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
            "\n",
            "cl100k_base: 9 tokens\n",
            "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
            "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Đếm tokens cho lệnh gọi API trò chuyện\n",
        "\n",
        "Mô hình ChatGPT như `gpt-3.5-turbo` và `gpt-4` sử tokens theo cách tương tự như các mô hình hoàn thành cũ hơn, nhưng bởi vì định dạng message-based của họ , khó hơn để đếm có bao nhiêu tokens sẽ được sử dụng bởi một cuộc trò chuyện.\n",
        "\n",
        "Dưới đây là một hàm ví dụ để đếm tokens cho các tin nhắn được chuyển đến `gpt-3.5-turbo-0301` hoặc `gpt-4-0314`.\n",
        "\n",
        "Lưu ý rằng cách chính xác mà tokens được tính từ tin nhắn có thể thay đổi từ mô hình này sang mô hình khác. Hãy xem xét số lượng từ ở hàm dưới đây là ước tính, không phải là một sự đảm bảo vượt thời gian."
      ],
      "metadata": {
        "id": "s8r9tUKPxJBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo\":\n",
        "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
        "    elif model == \"gpt-4\":\n",
        "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif model == \"gpt-4-0314\":\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n",
        "# Đoạn code trên định nghĩa hàm \"num_tokens_from_messages\" để tính toán số lượng tokens\n",
        "# (đơn vị cơ bản trong xử lý ngôn ngữ tự nhiên) được sử dụng bởi một danh sách các tin nhắn.\n",
        "# Đầu vào của hàm là một danh sách các tin nhắn và tên của mô hình ngôn ngữ được sử dụng\n",
        "# để mã hóa văn bản thành tokens. Hàm kiểm tra xem mô hình có được hỗ trợ hay không, nếu không,\n",
        "# hàm sử dụng mã hóa \"cl100k_base\" để mã hóa các văn bản. Hàm sử dụng các giá trị cụ thể cho\n",
        "# số lượng tokens được sử dụng bởi mỗi mô hình và sau đó tính tổng số tokens cho tất cả các\n",
        "# tin nhắn trong danh sách. Cuối cùng, hàm trả về tổng số tokens."
      ],
      "metadata": {
        "id": "rKBJ9Y9HxIfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's verify the function above matches the OpenAI API response\n",
        "\n",
        "import openai\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for model in [\"gpt-3.5-turbo-0301\"]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "    openai.api_key = (\"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\")\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=example_messages,\n",
        "        temperature=0,\n",
        "        max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output\n",
        "    )\n",
        "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
        "    print()\n",
        "# Đoạn code trên được sử dụng để so sánh số lượng token được sử dụng trong một đoạn hội thoại\n",
        "# giữa hai đối tượng (user và assistant) bằng hai cách khác nhau:\n",
        "# Sử dụng hàm num_tokens_from_messages để tính toán số lượng token được sử dụng dựa trên encoding của một model ngôn ngữ cụ thể.\n",
        "# Sử dụng API của OpenAI để tính toán số lượng token được sử dụng bằng cách tạo một prompt với model và thông tin về hội thoại\n",
        "# và lấy số lượng token được trả về từ API.\n",
        "# Đoạn code trên chạy ví dụ với các thông tin hội thoại được định nghĩa sẵn trong biến example_messages\n",
        "# và sử dụng một model ngôn ngữ cụ thể là gpt-3.5-turbo-0301. Kết quả số lượng token được sử dụng sẽ được\n",
        "# in ra để so sánh giữa hai phương pháp tính toán token."
      ],
      "metadata": {
        "id": "9bbfmkgPy6tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4236d47c-db47-48ae-cae1-b617c21b494a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-0301\n",
            "127 prompt tokens counted by num_tokens_from_messages().\n",
            "127 prompt tokens counted by the OpenAI API.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6_pHnDvu0Yu"
      },
      "source": [
        "# Làm thế nào để hoàn thành luồng\n",
        "\n",
        "Theo mặc định, khi bạn yêu cầu hoàn thành từ OpenAI, toàn bộ quá trình hoàn thành sẽ được tạo trước khi được gửi lại trong một phản hồi.\n",
        "\n",
        "Nếu bạn đang tạo các lần hoàn thành dài, thì việc chờ phản hồi có thể mất nhiều giây.\n",
        "\n",
        "Để nhận được phản hồi sớm hơn, bạn có thể 'truyền phát' hoàn thành khi nó được tạo. Điều này cho phép bạn bắt đầu in hoặc xử lý phần đầu của quá trình hoàn tất trước khi quá trình hoàn tất đầy đủ kết thúc.\n",
        "\n",
        "Để truyền phát hoàn thành, hãy đặt `stream=True` khi gọi các điểm cuối hoàn thành hoặc hoàn thành trò chuyện. Thao tác này sẽ trả về một đối tượng truyền ngược phản hồi dưới dạng [sự kiện do máy chủ gửi chỉ dành cho dữ liệu](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format ). Trích xuất các đoạn từ trường `delta` thay vì trường `message`.\n",
        "\n",
        "## Nhược điểm\n",
        "\n",
        "Lưu ý rằng việc sử dụng `stream=True` trong ứng dụng sản xuất sẽ khiến việc kiểm duyệt nội dung của phần hoàn thành trở nên khó khăn hơn, vì phần hoàn thành một phần có thể khó đánh giá hơn. có liên quan đến [cách sử dụng được phê duyệt](https://beta.openai.com/docs/usage-guidelines).\n",
        "\n",
        "Một nhược điểm nhỏ khác của phản hồi phát trực tuyến là phản hồi không còn bao gồm trường `mức sử dụng` để cho bạn biết có bao nhiêu mã thông báo đã được sử dụng. Sau khi nhận và kết hợp tất cả các câu trả lời, bạn có thể tự tính toán giá trị này bằng cách sử dụng [`tiktoken`](How_to_count_tokens_with_tiktoken.ipynb).\n",
        "\n",
        "## Mã ví dụ\n",
        "\n",
        "Dưới đây, sổ ghi chép này cho thấy:\n",
        "1. Phản hồi hoàn thành cuộc trò chuyện điển hình trông như thế nào\n",
        "2. Phản hồi hoàn thành trò chơi trực tuyến trông như thế nào\n",
        "3. Tiết kiệm điện được bao nhiêu thời gian bằng cách phát trực tuyến hoàn chỉnh\n",
        "4. Cách để truyền phát hoàn thành không trò chuyện (được sử dụng bởi các mô hình cũ hơn như `text-davinci-003`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1kC0jezu0Yw"
      },
      "outputs": [],
      "source": [
        "# Thêm thư viện\n",
        "import openai  # for OpenAI API calls\n",
        "import time  # for measuring time duration of API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What a typical chat completion response looks like\n",
        "\n",
        "Với lệnh gọi API ChatCompletions điển hình, trước tiên, phản hồi được tính toán rồi trả về tất cả cùng một lúc."
      ],
      "metadata": {
        "id": "OJaDipsXyjkH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJecVA35u0Yx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e10b6d-27ec-43dc-9db2-232173fc8335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full response received 15.13 seconds after request\n",
            "Full response received:\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1680830475,\n",
            "  \"id\": \"chatcmpl-72V4tLJKJFZedUjShZ3lXGY8b7xVr\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 299,\n",
            "    \"prompt_tokens\": 37,\n",
            "    \"total_tokens\": 336\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Example of an OpenAI ChatCompletion request\n",
        "# https://platform.openai.com/docs/guides/chat\n",
        "\n",
        "# record the time before the request is sent\n",
        "# Ghi lại thời gian trước khi yêu cầu được gửi đi\n",
        "start_time = time.time()\n",
        "\n",
        "# Gửi một yêu cầu ChatCompletion để đếm đến 100\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# calculate the time it took to receive the response\n",
        "# Tính thời gian để nhận được phản hồi\n",
        "response_time = time.time() - start_time\n",
        "\n",
        "# print the time delay and text received\n",
        "# In ra màn hình thời gian delay và text đã nhận được\n",
        "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
        "print(f\"Full response received:\\n{response}\")\n",
        "# Đoạn code này gửi một yêu cầu tới OpenAI API để thực hiện một trò chuyện bằng cách sử dụng\n",
        "# mô hình \"gpt-3.5-turbo\" với nội dung là \"Count to 100, with a comma between each number and\n",
        "# no newlines. E.g., 1, 2, 3, ...\". Kết quả được trả về sẽ là một chuỗi văn bản trong đó sẽ đếm\n",
        "# từ 1 đến 100 với dấu phẩy giữa mỗi số và không có ký tự xuống dòng. Thời gian phản hồi của\n",
        "# yêu cầu được tính và in ra kết quả."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK3eBVvMu0Yy"
      },
      "source": [
        "Câu trả lời có thể được trích xuất với `response['choices'][0]['message']`.\n",
        "\n",
        "Nội dung của câu trả lời có thể được trích xuất với `response['choices'][0]['message']['content']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbeLsYo4u0Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b582079-c5c9-49c8-efda-d282c592812a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted reply: \n",
            "{\n",
            "  \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\",\n",
            "  \"role\": \"assistant\"\n",
            "}\n",
            "Extracted content: \n",
            "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n"
          ]
        }
      ],
      "source": [
        "reply = response['choices'][0]['message']\n",
        "print(f\"Extracted reply: \\n{reply}\")\n",
        "\n",
        "reply_content = response['choices'][0]['message']['content']\n",
        "print(f\"Extracted content: \\n{reply_content}\")\n",
        "# Đoạn code này lấy nội dung phản hồi từ response của yêu cầu ChatCompletion và in nó ra màn hình.\n",
        "# Cụ thể, reply lấy nội dung phản hồi từ phần tử đầu tiên trong danh sách choices của response.\n",
        "# Sau đó, reply_content lấy nội dung của phần tử content từ nội dung phản hồi trên."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWRn1dBZu0Yz"
      },
      "source": [
        "### 2. Cách stream complete cuộc trò chuyện\n",
        "\n",
        "Với lệnh gọi API phát trực tuyến, phản hồi được gửi lại theo từng đoạn thông qua [luồng sự kiện](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format ). Trong Python, bạn có thể lặp lại các sự kiện này bằng vòng lặp `for`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5CYjgdau0Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fbefcf-497a-49e9-ddf9-a9ba2e44083e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"delta\": {\n",
            "        \"role\": \"assistant\"\n",
            "      },\n",
            "      \"finish_reason\": null,\n",
            "      \"index\": 0\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1680830543,\n",
            "  \"id\": \"chatcmpl-72V5zhKpTgxjNgixApdczY20x4ZyO\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion.chunk\"\n",
            "}\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"delta\": {\n",
            "        \"content\": \"Two\"\n",
            "      },\n",
            "      \"finish_reason\": null,\n",
            "      \"index\": 0\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1680830543,\n",
            "  \"id\": \"chatcmpl-72V5zhKpTgxjNgixApdczY20x4ZyO\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion.chunk\"\n",
            "}\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"delta\": {\n",
            "        \"content\": \".\"\n",
            "      },\n",
            "      \"finish_reason\": null,\n",
            "      \"index\": 0\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1680830543,\n",
            "  \"id\": \"chatcmpl-72V5zhKpTgxjNgixApdczY20x4ZyO\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion.chunk\"\n",
            "}\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"delta\": {},\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1680830543,\n",
            "  \"id\": \"chatcmpl-72V5zhKpTgxjNgixApdczY20x4ZyO\",\n",
            "  \"model\": \"gpt-3.5-turbo-0301\",\n",
            "  \"object\": \"chat.completion.chunk\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Example of an OpenAI ChatCompletion request with stream=True\n",
        "# https://platform.openai.com/docs/guides/chat\n",
        "\n",
        "# a ChatCompletion request\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    stream=True  # this time, we set stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk)\n",
        "# Đoạn code trên là một ví dụ về cách sử dụng OpenAI API để gửi yêu cầu ChatCompletion\n",
        "# với stream=True để nhận dữ liệu theo kiểu dòng liên tục. Khi đó, phản hồi của OpenAI API\n",
        "# sẽ được trả về dưới dạng một đối tượng streaming, và ta cần duyệt qua từng chunk để lấy\n",
        "# dữ liệu. Cụ thể, đoạn code này sử dụng mô hình gpt-3.5-turbo để trả lời câu hỏi \"What's 1+1?\n",
        "# Answer in one word.\" và in ra phản hồi dưới dạng các chunk được trả về."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KcwGOrbu0Yz"
      },
      "source": [
        "Như mọi người thấy ở trên, phản hồi trực tuyến có một `delta` field chứ không phải là một `message` field. `delta` có thể giữ lại một số thứ như:\n",
        "- a role token (e.g., `{\"role\": \"assistant\"}`)\n",
        "- a content token (e.g., `{\"content\": \"\\n\\n\"}`)\n",
        "- nothing (e.g., `{}`), when the stream is over"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck31w39eu0Y0"
      },
      "source": [
        "### 3. Tiết kiệm bao nhiêu thời gian bằng cách stream completion\n",
        "\n",
        "Bây giờ, hãy yêu cầu `gpt-3.5-turbo` đếm lại đến 100 và xem mất bao lâu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq21tKiyu0Y0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9167ef-873c-4631-c575-e260c6782467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message received 0.46 seconds after request: {\n",
            "  \"role\": \"assistant\"\n",
            "}\n",
            "Message received 0.46 seconds after request: {\n",
            "  \"content\": \"1\"\n",
            "}\n",
            "Message received 0.50 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 0.55 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 0.61 seconds after request: {\n",
            "  \"content\": \"2\"\n",
            "}\n",
            "Message received 0.65 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 0.70 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 0.75 seconds after request: {\n",
            "  \"content\": \"3\"\n",
            "}\n",
            "Message received 0.80 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 0.84 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 0.90 seconds after request: {\n",
            "  \"content\": \"4\"\n",
            "}\n",
            "Message received 0.94 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 0.99 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.04 seconds after request: {\n",
            "  \"content\": \"5\"\n",
            "}\n",
            "Message received 1.09 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 1.19 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.19 seconds after request: {\n",
            "  \"content\": \"6\"\n",
            "}\n",
            "Message received 1.24 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 1.29 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.34 seconds after request: {\n",
            "  \"content\": \"7\"\n",
            "}\n",
            "Message received 1.39 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 1.43 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.48 seconds after request: {\n",
            "  \"content\": \"8\"\n",
            "}\n",
            "Message received 1.54 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 1.59 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.68 seconds after request: {\n",
            "  \"content\": \"9\"\n",
            "}\n",
            "Message received 1.73 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 1.77 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.80 seconds after request: {\n",
            "  \"content\": \"10\"\n",
            "}\n",
            "Message received 1.84 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 1.92 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 1.97 seconds after request: {\n",
            "  \"content\": \"11\"\n",
            "}\n",
            "Message received 1.99 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.04 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 2.10 seconds after request: {\n",
            "  \"content\": \"12\"\n",
            "}\n",
            "Message received 2.20 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.20 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 2.24 seconds after request: {\n",
            "  \"content\": \"13\"\n",
            "}\n",
            "Message received 2.29 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.34 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 2.38 seconds after request: {\n",
            "  \"content\": \"14\"\n",
            "}\n",
            "Message received 2.43 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.48 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 2.56 seconds after request: {\n",
            "  \"content\": \"15\"\n",
            "}\n",
            "Message received 2.65 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.67 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 2.71 seconds after request: {\n",
            "  \"content\": \"16\"\n",
            "}\n",
            "Message received 2.78 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.82 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 2.86 seconds after request: {\n",
            "  \"content\": \"17\"\n",
            "}\n",
            "Message received 2.92 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 2.98 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.00 seconds after request: {\n",
            "  \"content\": \"18\"\n",
            "}\n",
            "Message received 3.06 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 3.11 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.17 seconds after request: {\n",
            "  \"content\": \"19\"\n",
            "}\n",
            "Message received 3.21 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 3.26 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.30 seconds after request: {\n",
            "  \"content\": \"20\"\n",
            "}\n",
            "Message received 3.36 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 3.40 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.48 seconds after request: {\n",
            "  \"content\": \"21\"\n",
            "}\n",
            "Message received 3.51 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 3.55 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.62 seconds after request: {\n",
            "  \"content\": \"22\"\n",
            "}\n",
            "Message received 3.65 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 3.71 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.76 seconds after request: {\n",
            "  \"content\": \"23\"\n",
            "}\n",
            "Message received 3.82 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 3.87 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 3.91 seconds after request: {\n",
            "  \"content\": \"24\"\n",
            "}\n",
            "Message received 3.96 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.01 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.06 seconds after request: {\n",
            "  \"content\": \"25\"\n",
            "}\n",
            "Message received 4.11 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.16 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.20 seconds after request: {\n",
            "  \"content\": \"26\"\n",
            "}\n",
            "Message received 4.26 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.30 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.35 seconds after request: {\n",
            "  \"content\": \"27\"\n",
            "}\n",
            "Message received 4.40 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.45 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.50 seconds after request: {\n",
            "  \"content\": \"28\"\n",
            "}\n",
            "Message received 4.54 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.59 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.66 seconds after request: {\n",
            "  \"content\": \"29\"\n",
            "}\n",
            "Message received 4.69 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.74 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.78 seconds after request: {\n",
            "  \"content\": \"30\"\n",
            "}\n",
            "Message received 4.86 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 4.87 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 4.92 seconds after request: {\n",
            "  \"content\": \"31\"\n",
            "}\n",
            "Message received 4.98 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.01 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.10 seconds after request: {\n",
            "  \"content\": \"32\"\n",
            "}\n",
            "Message received 5.13 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.17 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.26 seconds after request: {\n",
            "  \"content\": \"33\"\n",
            "}\n",
            "Message received 5.32 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.34 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.39 seconds after request: {\n",
            "  \"content\": \"34\"\n",
            "}\n",
            "Message received 5.44 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.48 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.54 seconds after request: {\n",
            "  \"content\": \"35\"\n",
            "}\n",
            "Message received 5.59 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.66 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.69 seconds after request: {\n",
            "  \"content\": \"36\"\n",
            "}\n",
            "Message received 5.74 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.78 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.83 seconds after request: {\n",
            "  \"content\": \"37\"\n",
            "}\n",
            "Message received 5.88 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 5.93 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 5.97 seconds after request: {\n",
            "  \"content\": \"38\"\n",
            "}\n",
            "Message received 6.02 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.07 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 6.12 seconds after request: {\n",
            "  \"content\": \"39\"\n",
            "}\n",
            "Message received 6.17 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.22 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 6.28 seconds after request: {\n",
            "  \"content\": \"40\"\n",
            "}\n",
            "Message received 6.32 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.37 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 6.43 seconds after request: {\n",
            "  \"content\": \"41\"\n",
            "}\n",
            "Message received 6.48 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.52 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 6.58 seconds after request: {\n",
            "  \"content\": \"42\"\n",
            "}\n",
            "Message received 6.62 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.68 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 6.71 seconds after request: {\n",
            "  \"content\": \"43\"\n",
            "}\n",
            "Message received 6.79 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.84 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 6.87 seconds after request: {\n",
            "  \"content\": \"44\"\n",
            "}\n",
            "Message received 6.91 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 6.96 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.01 seconds after request: {\n",
            "  \"content\": \"45\"\n",
            "}\n",
            "Message received 7.06 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 7.10 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.15 seconds after request: {\n",
            "  \"content\": \"46\"\n",
            "}\n",
            "Message received 7.20 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 7.24 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.29 seconds after request: {\n",
            "  \"content\": \"47\"\n",
            "}\n",
            "Message received 7.34 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 7.40 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.45 seconds after request: {\n",
            "  \"content\": \"48\"\n",
            "}\n",
            "Message received 7.50 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 7.55 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.60 seconds after request: {\n",
            "  \"content\": \"49\"\n",
            "}\n",
            "Message received 7.66 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 7.70 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.75 seconds after request: {\n",
            "  \"content\": \"50\"\n",
            "}\n",
            "Message received 7.80 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 7.85 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 7.94 seconds after request: {\n",
            "  \"content\": \"51\"\n",
            "}\n",
            "Message received 7.95 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.04 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.08 seconds after request: {\n",
            "  \"content\": \"52\"\n",
            "}\n",
            "Message received 8.10 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.15 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.21 seconds after request: {\n",
            "  \"content\": \"53\"\n",
            "}\n",
            "Message received 8.26 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.31 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.35 seconds after request: {\n",
            "  \"content\": \"54\"\n",
            "}\n",
            "Message received 8.43 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.46 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.51 seconds after request: {\n",
            "  \"content\": \"55\"\n",
            "}\n",
            "Message received 8.57 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.63 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.69 seconds after request: {\n",
            "  \"content\": \"56\"\n",
            "}\n",
            "Message received 8.74 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.79 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.84 seconds after request: {\n",
            "  \"content\": \"57\"\n",
            "}\n",
            "Message received 8.90 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 8.95 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 8.99 seconds after request: {\n",
            "  \"content\": \"58\"\n",
            "}\n",
            "Message received 9.07 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.09 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 9.14 seconds after request: {\n",
            "  \"content\": \"59\"\n",
            "}\n",
            "Message received 9.20 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.34 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 9.34 seconds after request: {\n",
            "  \"content\": \"60\"\n",
            "}\n",
            "Message received 9.36 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.38 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 9.43 seconds after request: {\n",
            "  \"content\": \"61\"\n",
            "}\n",
            "Message received 9.48 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.53 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 9.58 seconds after request: {\n",
            "  \"content\": \"62\"\n",
            "}\n",
            "Message received 9.62 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.68 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 9.72 seconds after request: {\n",
            "  \"content\": \"63\"\n",
            "}\n",
            "Message received 9.77 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.81 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 9.87 seconds after request: {\n",
            "  \"content\": \"64\"\n",
            "}\n",
            "Message received 9.92 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 9.96 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.02 seconds after request: {\n",
            "  \"content\": \"65\"\n",
            "}\n",
            "Message received 10.07 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 10.11 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.20 seconds after request: {\n",
            "  \"content\": \"66\"\n",
            "}\n",
            "Message received 10.23 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 10.28 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.33 seconds after request: {\n",
            "  \"content\": \"67\"\n",
            "}\n",
            "Message received 10.39 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 10.47 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.51 seconds after request: {\n",
            "  \"content\": \"68\"\n",
            "}\n",
            "Message received 10.56 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 10.62 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.66 seconds after request: {\n",
            "  \"content\": \"69\"\n",
            "}\n",
            "Message received 10.70 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 10.75 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.81 seconds after request: {\n",
            "  \"content\": \"70\"\n",
            "}\n",
            "Message received 10.88 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 10.91 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 10.98 seconds after request: {\n",
            "  \"content\": \"71\"\n",
            "}\n",
            "Message received 11.03 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 11.09 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 11.13 seconds after request: {\n",
            "  \"content\": \"72\"\n",
            "}\n",
            "Message received 11.17 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 11.26 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 11.29 seconds after request: {\n",
            "  \"content\": \"73\"\n",
            "}\n",
            "Message received 11.36 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 11.38 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 11.44 seconds after request: {\n",
            "  \"content\": \"74\"\n",
            "}\n",
            "Message received 11.50 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 11.57 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 11.64 seconds after request: {\n",
            "  \"content\": \"75\"\n",
            "}\n",
            "Message received 11.67 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 11.72 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 11.77 seconds after request: {\n",
            "  \"content\": \"76\"\n",
            "}\n",
            "Message received 11.84 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 11.88 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 11.93 seconds after request: {\n",
            "  \"content\": \"77\"\n",
            "}\n",
            "Message received 11.97 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 12.04 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 12.07 seconds after request: {\n",
            "  \"content\": \"78\"\n",
            "}\n",
            "Message received 12.11 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 12.16 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 12.27 seconds after request: {\n",
            "  \"content\": \"79\"\n",
            "}\n",
            "Message received 12.29 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 12.35 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 12.39 seconds after request: {\n",
            "  \"content\": \"80\"\n",
            "}\n",
            "Message received 12.42 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 12.47 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 12.53 seconds after request: {\n",
            "  \"content\": \"81\"\n",
            "}\n",
            "Message received 12.59 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 12.64 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 12.69 seconds after request: {\n",
            "  \"content\": \"82\"\n",
            "}\n",
            "Message received 12.76 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 12.83 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 12.91 seconds after request: {\n",
            "  \"content\": \"83\"\n",
            "}\n",
            "Message received 12.97 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.01 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 13.08 seconds after request: {\n",
            "  \"content\": \"84\"\n",
            "}\n",
            "Message received 13.14 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.18 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 13.23 seconds after request: {\n",
            "  \"content\": \"85\"\n",
            "}\n",
            "Message received 13.29 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.35 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 13.39 seconds after request: {\n",
            "  \"content\": \"86\"\n",
            "}\n",
            "Message received 13.45 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.48 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 13.56 seconds after request: {\n",
            "  \"content\": \"87\"\n",
            "}\n",
            "Message received 13.58 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.63 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 13.70 seconds after request: {\n",
            "  \"content\": \"88\"\n",
            "}\n",
            "Message received 13.76 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.84 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 13.89 seconds after request: {\n",
            "  \"content\": \"89\"\n",
            "}\n",
            "Message received 13.95 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 13.99 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.04 seconds after request: {\n",
            "  \"content\": \"90\"\n",
            "}\n",
            "Message received 14.14 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 14.15 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.20 seconds after request: {\n",
            "  \"content\": \"91\"\n",
            "}\n",
            "Message received 14.26 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 14.30 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.36 seconds after request: {\n",
            "  \"content\": \"92\"\n",
            "}\n",
            "Message received 14.41 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 14.46 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.53 seconds after request: {\n",
            "  \"content\": \"93\"\n",
            "}\n",
            "Message received 14.60 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 14.64 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.68 seconds after request: {\n",
            "  \"content\": \"94\"\n",
            "}\n",
            "Message received 14.78 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 14.78 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.84 seconds after request: {\n",
            "  \"content\": \"95\"\n",
            "}\n",
            "Message received 14.92 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 14.96 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 14.99 seconds after request: {\n",
            "  \"content\": \"96\"\n",
            "}\n",
            "Message received 15.06 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 15.11 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 15.16 seconds after request: {\n",
            "  \"content\": \"97\"\n",
            "}\n",
            "Message received 15.21 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 15.26 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 15.31 seconds after request: {\n",
            "  \"content\": \"98\"\n",
            "}\n",
            "Message received 15.37 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 15.41 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 15.46 seconds after request: {\n",
            "  \"content\": \"99\"\n",
            "}\n",
            "Message received 15.51 seconds after request: {\n",
            "  \"content\": \",\"\n",
            "}\n",
            "Message received 15.56 seconds after request: {\n",
            "  \"content\": \" \"\n",
            "}\n",
            "Message received 15.62 seconds after request: {\n",
            "  \"content\": \"100\"\n",
            "}\n",
            "Message received 15.68 seconds after request: {\n",
            "  \"content\": \".\"\n",
            "}\n",
            "Message received 15.73 seconds after request: {}\n",
            "Full response received 15.73 seconds after request\n",
            "Full conversation received: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n"
          ]
        }
      ],
      "source": [
        "# Example of an OpenAI ChatCompletion request with stream=True\n",
        "# https://platform.openai.com/docs/guides/chat\n",
        "\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a ChatCompletion request to count to 100\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    stream=True  # again, we set stream=True\n",
        ")\n",
        "\n",
        "# create variables to collect the stream of chunks\n",
        "collected_chunks = []\n",
        "collected_messages = []\n",
        "# iterate through the stream of events\n",
        "for chunk in response:\n",
        "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
        "    collected_chunks.append(chunk)  # save the event response\n",
        "    chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
        "    collected_messages.append(chunk_message)  # save the message\n",
        "    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
        "full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
        "print(f\"Full conversation received: {full_reply_content}\")\n",
        "# Đoạn code này là một ví dụ về yêu cầu ChatCompletion với stream=True để nhận dữ liệu dạng stream\n",
        "# (được chia nhỏ thành các phần nhỏ hơn để truyền qua mạng) từ API OpenAI. Nó gửi yêu cầu để đếm\n",
        "# đến 100 và lưu trữ các phần nhỏ của phản hồi trong một danh sách, sau đó nối chúng lại để tạo thành\n",
        "# phản hồi đầy đủ. Nó cũng tính thời gian giữa các phần nhỏ của phản hồi và in ra màn hình."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2Gn9nMtu0Y0"
      },
      "source": [
        "#### So sánh thời gian hoàn thành\n",
        "\n",
        "Trong ví dụ trên, cả hai yêu cầu mất khoảng thời gian khá lâu để hoàn thành. Thời gian yêu cầu sẽ khác nhau tùy thuộc vào tải và các yếu tố ngẫu nhiên khác.\n",
        "\n",
        "Tuy nhiên, với yêu cầu phát trực tuyến, chúng ta đã nhận được mã thông báo đầu tiên sau 0,42 giây và các mã thông báo tiếp theo cứ sau ~0,05-0,1 giây"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66W-Y7kTu0Y0"
      },
      "source": [
        "\n",
        "### 4. Cách phát trực tuyến các lần hoàn thành không trò chuyện (được sử dụng bởi các mô hình cũ hơn như `text-davinci-003`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2YwbMsQu0Y0"
      },
      "source": [
        "#### Một yêu cầu hoàn thành điển hình\n",
        "\n",
        "Với một lệnh gọi API Hoàn thành điển hình, văn bản được tính toán trước rồi sau đó được trả về tất cả cùng một lúc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffie9BfYu0Y0",
        "outputId": "e5e7488d-1317-468c-df9a-6ade2ff43e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full response received 3.75 seconds after request\n",
            "Full text received: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n"
          ]
        }
      ],
      "source": [
        "# Example of an OpenAI Completion request\n",
        "# https://beta.openai.com/docs/api-reference/completions/create\n",
        "\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a Completion request to count to 100\n",
        "response = openai.Completion.create(\n",
        "    model='text-davinci-002',\n",
        "    prompt='1,2,3,',\n",
        "    max_tokens=193,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# calculate the time it took to receive the response\n",
        "response_time = time.time() - start_time\n",
        "\n",
        "# extract the text from the response\n",
        "completion_text = response['choices'][0]['text']\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
        "print(f\"Full text received: {completion_text}\")\n",
        "# Đoạn code trên là ví dụ về một yêu cầu OpenAI Completion. Nó sử dụng API của OpenAI để\n",
        "# tạo ra một đoạn văn bản mới bằng cách đưa ra một prompt (đoạn văn bản bắt đầu) và các\n",
        "# tham số khác như model, max_tokens và temperature. Kết quả trả về là một dictionary chứa\n",
        "# các phần tử, trong đó phần tử \"choices\" là một mảng chứa kết quả đầu ra với các thông tin\n",
        "# khác nhau như text, score, ... Ở đoạn code trên, chúng ta in ra màn hình thời gian delay \n",
        "# và đoạn văn bản đã được trả về bởi API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFcPOaShu0Y1"
      },
      "source": [
        "#### Yêu cầu hoàn thành phát trực tuyến\n",
        "\n",
        "Với lệnh gọi API Hoàn thành phát trực tuyến, văn bản được gửi lại qua một loạt sự kiện. Trong Python, bạn có thể lặp lại các sự kiện này bằng vòng lặp `for`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyECjq9mu0Y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fa7cf2-7975-43af-d252-04d08bea2463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text received: 4 (0.21 seconds after request)\n",
            "Text received: , (0.22 seconds after request)\n",
            "Text received: 5 (0.23 seconds after request)\n",
            "Text received: , (0.24 seconds after request)\n",
            "Text received: 6 (0.26 seconds after request)\n",
            "Text received: , (0.27 seconds after request)\n",
            "Text received: 7 (0.27 seconds after request)\n",
            "Text received: , (0.28 seconds after request)\n",
            "Text received: 8 (0.29 seconds after request)\n",
            "Text received: , (0.30 seconds after request)\n",
            "Text received: 9 (0.31 seconds after request)\n",
            "Text received: , (0.32 seconds after request)\n",
            "Text received: 10 (0.34 seconds after request)\n",
            "Text received: , (0.35 seconds after request)\n",
            "Text received: 11 (0.36 seconds after request)\n",
            "Text received: , (0.37 seconds after request)\n",
            "Text received: 12 (0.38 seconds after request)\n",
            "Text received: , (0.39 seconds after request)\n",
            "Text received: 13 (0.40 seconds after request)\n",
            "Text received: , (0.41 seconds after request)\n",
            "Text received: 14 (0.42 seconds after request)\n",
            "Text received: , (0.44 seconds after request)\n",
            "Text received: 15 (0.44 seconds after request)\n",
            "Text received: , (0.45 seconds after request)\n",
            "Text received: 16 (0.47 seconds after request)\n",
            "Text received: , (0.48 seconds after request)\n",
            "Text received: 17 (0.49 seconds after request)\n",
            "Text received: , (0.50 seconds after request)\n",
            "Text received: 18 (0.51 seconds after request)\n",
            "Text received: , (0.52 seconds after request)\n",
            "Text received: 19 (0.53 seconds after request)\n",
            "Text received: , (0.54 seconds after request)\n",
            "Text received: 20 (0.55 seconds after request)\n",
            "Text received: , (0.56 seconds after request)\n",
            "Text received: 21 (0.57 seconds after request)\n",
            "Text received: , (0.58 seconds after request)\n",
            "Text received: 22 (0.59 seconds after request)\n",
            "Text received: , (0.60 seconds after request)\n",
            "Text received: 23 (0.61 seconds after request)\n",
            "Text received: , (0.62 seconds after request)\n",
            "Text received: 24 (0.63 seconds after request)\n",
            "Text received: , (0.64 seconds after request)\n",
            "Text received: 25 (0.65 seconds after request)\n",
            "Text received: , (0.66 seconds after request)\n",
            "Text received: 26 (0.67 seconds after request)\n",
            "Text received: , (0.68 seconds after request)\n",
            "Text received: 27 (0.69 seconds after request)\n",
            "Text received: , (0.70 seconds after request)\n",
            "Text received: 28 (0.71 seconds after request)\n",
            "Text received: , (0.72 seconds after request)\n",
            "Text received: 29 (0.73 seconds after request)\n",
            "Text received: , (0.74 seconds after request)\n",
            "Text received: 30 (0.75 seconds after request)\n",
            "Text received: , (0.76 seconds after request)\n",
            "Text received: 31 (0.78 seconds after request)\n",
            "Text received: , (0.78 seconds after request)\n",
            "Text received: 32 (0.79 seconds after request)\n",
            "Text received: , (0.81 seconds after request)\n",
            "Text received: 33 (0.81 seconds after request)\n",
            "Text received: , (0.82 seconds after request)\n",
            "Text received: 34 (0.83 seconds after request)\n",
            "Text received: , (0.84 seconds after request)\n",
            "Text received: 35 (0.86 seconds after request)\n",
            "Text received: , (0.86 seconds after request)\n",
            "Text received: 36 (0.87 seconds after request)\n",
            "Text received: , (0.89 seconds after request)\n",
            "Text received: 37 (0.90 seconds after request)\n",
            "Text received: , (0.91 seconds after request)\n",
            "Text received: 38 (0.92 seconds after request)\n",
            "Text received: , (0.93 seconds after request)\n",
            "Text received: 39 (0.94 seconds after request)\n",
            "Text received: , (0.95 seconds after request)\n",
            "Text received: 40 (0.96 seconds after request)\n",
            "Text received: , (0.97 seconds after request)\n",
            "Text received: 41 (0.98 seconds after request)\n",
            "Text received: , (0.99 seconds after request)\n",
            "Text received: 42 (1.00 seconds after request)\n",
            "Text received: , (1.01 seconds after request)\n",
            "Text received: 43 (1.02 seconds after request)\n",
            "Text received: , (1.03 seconds after request)\n",
            "Text received: 44 (1.04 seconds after request)\n",
            "Text received: , (1.05 seconds after request)\n",
            "Text received: 45 (1.06 seconds after request)\n",
            "Text received: , (1.07 seconds after request)\n",
            "Text received: 46 (1.08 seconds after request)\n",
            "Text received: , (1.09 seconds after request)\n",
            "Text received: 47 (1.10 seconds after request)\n",
            "Text received: , (1.11 seconds after request)\n",
            "Text received: 48 (1.12 seconds after request)\n",
            "Text received: , (1.13 seconds after request)\n",
            "Text received: 49 (1.14 seconds after request)\n",
            "Text received: , (1.15 seconds after request)\n",
            "Text received: 50 (1.16 seconds after request)\n",
            "Text received: , (1.17 seconds after request)\n",
            "Text received: 51 (1.18 seconds after request)\n",
            "Text received: , (1.19 seconds after request)\n",
            "Text received: 52 (1.21 seconds after request)\n",
            "Text received: , (1.22 seconds after request)\n",
            "Text received: 53 (1.22 seconds after request)\n",
            "Text received: , (1.23 seconds after request)\n",
            "Text received: 54 (1.24 seconds after request)\n",
            "Text received: , (1.25 seconds after request)\n",
            "Text received: 55 (1.26 seconds after request)\n",
            "Text received: , (1.27 seconds after request)\n",
            "Text received: 56 (1.29 seconds after request)\n",
            "Text received: , (1.30 seconds after request)\n",
            "Text received: 57 (1.31 seconds after request)\n",
            "Text received: , (1.32 seconds after request)\n",
            "Text received: 58 (1.33 seconds after request)\n",
            "Text received: , (1.34 seconds after request)\n",
            "Text received: 59 (1.35 seconds after request)\n",
            "Text received: , (1.36 seconds after request)\n",
            "Text received: 60 (1.37 seconds after request)\n",
            "Text received: , (1.38 seconds after request)\n",
            "Text received: 61 (1.39 seconds after request)\n",
            "Text received: , (1.41 seconds after request)\n",
            "Text received: 62 (1.41 seconds after request)\n",
            "Text received: , (1.42 seconds after request)\n",
            "Text received: 63 (1.43 seconds after request)\n",
            "Text received: , (1.44 seconds after request)\n",
            "Text received: 64 (1.45 seconds after request)\n",
            "Text received: , (1.46 seconds after request)\n",
            "Text received: 65 (1.47 seconds after request)\n",
            "Text received: , (1.49 seconds after request)\n",
            "Text received: 66 (1.49 seconds after request)\n",
            "Text received: , (1.50 seconds after request)\n",
            "Text received: 67 (1.51 seconds after request)\n",
            "Text received: , (1.52 seconds after request)\n",
            "Text received: 68 (1.53 seconds after request)\n",
            "Text received: , (1.55 seconds after request)\n",
            "Text received: 69 (1.56 seconds after request)\n",
            "Text received: , (1.57 seconds after request)\n",
            "Text received: 70 (1.58 seconds after request)\n",
            "Text received: , (1.59 seconds after request)\n",
            "Text received: 71 (1.60 seconds after request)\n",
            "Text received: , (1.60 seconds after request)\n",
            "Text received: 72 (1.62 seconds after request)\n",
            "Text received: , (1.63 seconds after request)\n",
            "Text received: 73 (1.64 seconds after request)\n",
            "Text received: , (1.66 seconds after request)\n",
            "Text received: 74 (1.66 seconds after request)\n",
            "Text received: , (1.67 seconds after request)\n",
            "Text received: 75 (1.68 seconds after request)\n",
            "Text received: , (1.69 seconds after request)\n",
            "Text received: 76 (1.70 seconds after request)\n",
            "Text received: , (1.71 seconds after request)\n",
            "Text received: 77 (1.72 seconds after request)\n",
            "Text received: , (1.73 seconds after request)\n",
            "Text received: 78 (1.74 seconds after request)\n",
            "Text received: , (1.76 seconds after request)\n",
            "Text received: 79 (1.76 seconds after request)\n",
            "Text received: , (1.77 seconds after request)\n",
            "Text received: 80 (1.78 seconds after request)\n",
            "Text received: , (1.79 seconds after request)\n",
            "Text received: 81 (1.80 seconds after request)\n",
            "Text received: , (1.81 seconds after request)\n",
            "Text received: 82 (1.82 seconds after request)\n",
            "Text received: , (1.83 seconds after request)\n",
            "Text received: 83 (1.84 seconds after request)\n",
            "Text received: , (1.85 seconds after request)\n",
            "Text received: 84 (1.87 seconds after request)\n",
            "Text received: , (1.88 seconds after request)\n",
            "Text received: 85 (1.89 seconds after request)\n",
            "Text received: , (1.90 seconds after request)\n",
            "Text received: 86 (1.91 seconds after request)\n",
            "Text received: , (1.92 seconds after request)\n",
            "Text received: 87 (1.93 seconds after request)\n",
            "Text received: , (1.94 seconds after request)\n",
            "Text received: 88 (1.95 seconds after request)\n",
            "Text received: , (1.96 seconds after request)\n",
            "Text received: 89 (1.97 seconds after request)\n",
            "Text received: , (1.99 seconds after request)\n",
            "Text received: 90 (1.99 seconds after request)\n",
            "Text received: , (2.00 seconds after request)\n",
            "Text received: 91 (2.01 seconds after request)\n",
            "Text received: , (2.02 seconds after request)\n",
            "Text received: 92 (2.03 seconds after request)\n",
            "Text received: , (2.04 seconds after request)\n",
            "Text received: 93 (2.05 seconds after request)\n",
            "Text received: , (2.06 seconds after request)\n",
            "Text received: 94 (2.08 seconds after request)\n",
            "Text received: , (2.09 seconds after request)\n",
            "Text received: 95 (2.09 seconds after request)\n",
            "Text received: , (2.11 seconds after request)\n",
            "Text received: 96 (2.12 seconds after request)\n",
            "Text received: , (2.13 seconds after request)\n",
            "Text received: 97 (2.14 seconds after request)\n",
            "Text received: , (2.15 seconds after request)\n",
            "Text received: 98 (2.16 seconds after request)\n",
            "Text received: , (2.17 seconds after request)\n",
            "Text received: 99 (2.19 seconds after request)\n",
            "Text received: , (2.20 seconds after request)\n",
            "Text received: 100 (2.21 seconds after request)\n",
            "Full response received 2.21 seconds after request\n",
            "Full text received: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n"
          ]
        }
      ],
      "source": [
        "# Example of an OpenAI Completion request, using the stream=True option\n",
        "# https://beta.openai.com/docs/api-reference/completions/create\n",
        "\n",
        "# record the time before the request is sent\n",
        "start_time = time.time()\n",
        "\n",
        "# send a Completion request to count to 100\n",
        "response = openai.Completion.create(\n",
        "    model='text-davinci-002',\n",
        "    prompt='1,2,3,',\n",
        "    max_tokens=193,\n",
        "    temperature=0,\n",
        "    stream=True,  # this time, we set stream=True\n",
        ")\n",
        "\n",
        "# create variables to collect the stream of events\n",
        "collected_events = []\n",
        "completion_text = ''\n",
        "# iterate through the stream of events\n",
        "for event in response:\n",
        "    event_time = time.time() - start_time  # calculate the time delay of the event\n",
        "    collected_events.append(event)  # save the event response\n",
        "    event_text = event['choices'][0]['text']  # extract the text\n",
        "    completion_text += event_text  # append the text\n",
        "    print(f\"Text received: {event_text} ({event_time:.2f} seconds after request)\")  # print the delay and text\n",
        "\n",
        "# print the time delay and text received\n",
        "print(f\"Full response received {event_time:.2f} seconds after request\")\n",
        "print(f\"Full text received: {completion_text}\")\n",
        "# Đoạn code này tương tự như đoạn code trước, nhưng sử dụng tùy chọn stream=True để nhận\n",
        "# dữ liệu theo luồng (stream) thay vì nhận dữ liệu đầy đủ ở một lần nhận duy nhất. Các chunks\n",
        "# (phần dữ liệu) được nhận về sẽ được lưu trữ và xử lý theo cách tương tự như đoạn code trước."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjMe7dwnu0Y1"
      },
      "source": [
        "#### So sánh thời gian\n",
        "\n",
        "Trong ví dụ trên, cả hai yêu cầu mất khoảng 3 giây để hoàn thành. Thời gian yêu cầu sẽ khác nhau tùy thuộc vào tải và các yếu tố ngẫu nhiên khác.\n",
        "\n",
        "Tuy nhiên, với yêu cầu phát trực tuyến, chúng tôi đã nhận được mã thông báo đầu tiên sau 0,46 giây và các mã thông báo tiếp theo cứ sau ~0,01-0,02 giây."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.9 ('openai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}